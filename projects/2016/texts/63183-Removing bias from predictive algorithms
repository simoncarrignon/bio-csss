Keywords
Machine learning, Justice policy

Summary
In many states across the US predictive algorithms are used in an effort to support judges in more objective decision making. These so called risk assessment algorithms are used to predict whether or not a person is likely to appear to court hearings, whether they are likely to repeat their criminal behavior after being released etc.. A recent report by ProPublica shows that one of these algorithms, which is in use in different states, is heavily biased against Afro-Americans. 

We are wondering whether there is a systematic way of removing such a bias, either by modifying the algorithm after training, or by systematically varying the set of criteria which is used to inform the algorithm in order to remove the bias without negatively affecting its predictive power.

A dataset of 7000 convicts and their criminal history after release from prison is available [online].

More generally, this could be used to avoid identifiability of specific groups in situations where such algorithms are used. This could apply for example to online shopping, in order to avoid discrimination.

Group Contact
Lars Hubatsch (lars.hubatsch@crick.ac.uk)

Interested Participants
Ulya Bayram
Nicole
Rudi Minxha
